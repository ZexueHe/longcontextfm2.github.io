---
layout: page
title: home
permalink: /
title: <h3  align="center">Workshop on Instruction Tuning and Instruction Following</h3>
nav_order: 1
---

<h2  align="center">Workshop on Instruction Tuning and Instruction Following</h2><br>
<p align="center">
@ NeurIPS 2023, Dec 15 or 16<br>
New Orleans, Louisiana, United States<br><br>

Quick Links:
<a href="/">Twitter</a> | <a href="/call">Call for Papers</a> |  <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/Instruction">OpenReview Portal</a> <br>
Program:
<a href="#speakers">Speakers</a> | <a href="#panelists">Panelists</a> | <a href="/organization">Organization</a> | <a href="/schedule">Schedule</a><br><br>
Contact Us: <a href="mailto:an-instructive-workshop@googlegroups.com">an-instructive-workshop@googlegroups.com</a>
</p>


<br>
Recent advancements in training large language models (LLMs) to follow "instructions" have significantly increased their ability to comprehend open-ended language commands, encompassing a wide range of needs, preferences, and values.

This remarkable transformation has led to the creation of phenomenal industrial models such as [GPT-4](https://arxiv.org/abs/2303.08774) and [Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/), as well as an increased focus within the open-source and research communities: creating new benchmark and resources [[1](https://aclanthology.org/2022.emnlp-main.340/),[2](https://arxiv.org/abs/2301.13688)], developing new training methods [[3](https://arxiv.org/abs/2203.02155),[4](https://arxiv.org/abs/2212.10560)], and understanding the limitations of these methods [[5](https://arxiv.org/abs/2109.01247)]. Furthermore, instruction following powered by LLMs has proven to be effective in multi-modal settings, with applications in image editing [[6](https://arxiv.org/abs/2211.09800)] and robotic command execution [[7](https://arxiv.org/abs/2204.01691)].

We organize this workshop to facilitate discussions on advancing instruction tuning methodologies and constructing general-purpose instruction-following models. We believe it is crucial to organize this workshop due to the prevalence of proprietary models with restricted access, thereby creating the need for an open platform to encourage discussions. Moreover, we aim to foster interdisciplinary collaboration by bringing together researchers from diverse fields such as natural language processing, computer vision, robotics, human-computer interaction, AI safety, among others, to share their latest findings and explore potential avenues for future research.

<br>

## Speakers

* [Sam Bowman](https://cims.nyu.edu/~sbowman/), Anthropic, New York University
* [Nazneen Rajani](https://www.nazneenrajani.com/), Hugging Face
* [Omer Levy](https://www.cs.tau.ac.il/~levyomer/), Tel Aviv University, Meta AI
* [Sara Hooker](https://www.sarahooker.me/), Cohere for AI
* [Fei Xia](https://fxia22.github.io/), Google DeepMind
* [Tatsunori Hashimoto](https://thashim.github.io/), Stanford University

<br>

## Panelists

Stay tuned! We plan to have two panels to encourage discussion!


<br>

## Organizers
* [Qinyuan Ye](http://yeqy.xyz/), University of Southern California
* [Yizhong Wang](https://homes.cs.washington.edu/~yizhongw/), University of Washington
* [Shayne Longpre](https://www.shaynelongpre.com/), Massachusetts Institute of Technology
* [Yao Fu](https://franxyao.github.io/), University of Edinburgh
* [Daniel Khashabi](https://danielkhashabi.com/), Johns Hopkins University

<br>

## Steering Committee
* [Hannaneh Hajishirzi](https://homes.cs.washington.edu/~hannaneh/), University of Washington, Allen Institute for AI
* [Xiang Ren](https://shanzhenren.github.io/), University of Southern California, Allen Institute for AI
* [Robin Jia](https://robinjia.github.io/), University of Southern California
